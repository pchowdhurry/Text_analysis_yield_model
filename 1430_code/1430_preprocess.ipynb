{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285b71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prottoyachowdhury/anaconda3/envs/text310/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use mps\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"TRANSFORMERS_NO_FLAX\"] = \"1\"\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from Extract import LabelSimilar \n",
    "from Extract import Extract \n",
    "from numpy.dtypes import StringDType\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA \n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import torch \n",
    "\n",
    "## Running on GPU \n",
    "device  = torch.device('cpu')\n",
    "\n",
    "\n",
    "\n",
    "model = SentenceTransformer(\"FinLang/finance-embeddings-investopedia\",device=device)\n",
    "folder_path = 'clean_merged_data/'\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-classification\", model=\"ProsusAI/finbert\", framework=\"pt\",device=device) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b0a8616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>is_gov</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r191212c_SNB</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>Introductory remarks, news conference</td>\n",
       "      <td>maechler</td>\n",
       "      <td>0</td>\n",
       "      <td>I would like to begin by reviewing development...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r220524a_FOMC</td>\n",
       "      <td>united states</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>Welcoming Remarks</td>\n",
       "      <td>powell</td>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, and welcome. It is a great pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r010402a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>Consolidation of the payment systems industry:...</td>\n",
       "      <td>london</td>\n",
       "      <td>0</td>\n",
       "      <td>I would first like to thank you for giving me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r171207a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>Making banks resolvable: the key to making res...</td>\n",
       "      <td>gracie</td>\n",
       "      <td>0</td>\n",
       "      <td>Resolution has come a long way since G20 Leade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r170913a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2017-09-13</td>\n",
       "      <td>Karl-Otto Pöhl Lecture</td>\n",
       "      <td>praet</td>\n",
       "      <td>0</td>\n",
       "      <td>The monetary policy measures introduced by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7716</th>\n",
       "      <td>r171214a_SNB</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>Introductory remarks, news conference</td>\n",
       "      <td>zurbrugg</td>\n",
       "      <td>0</td>\n",
       "      <td>In my remarks today, I would like to address s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>r991019a_BOJ</td>\n",
       "      <td>japan</td>\n",
       "      <td>1999-10-19</td>\n",
       "      <td>On Recent Monetary Policy</td>\n",
       "      <td>yamaguchi</td>\n",
       "      <td>0</td>\n",
       "      <td>I am honored to be invited to this conference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7718</th>\n",
       "      <td>r131122a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>Opening speech at the European Banking Congres...</td>\n",
       "      <td>draghi</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladies and gentlemen, Thank you for inviting m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7719</th>\n",
       "      <td>r970313a_FOMC</td>\n",
       "      <td>united states</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>we</td>\n",
       "      <td>rivlin</td>\n",
       "      <td>0</td>\n",
       "      <td>I am extremely pleased to have the opportunity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7720</th>\n",
       "      <td>r220428a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>Macropru</td>\n",
       "      <td>breeden</td>\n",
       "      <td>0</td>\n",
       "      <td>Dimitri Demekas and Paul Tucker for helpful di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7721 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          reference         country        date  \\\n",
       "0      r191212c_SNB     switzerland  2019-12-12   \n",
       "1     r220524a_FOMC   united states  2022-05-24   \n",
       "2      r010402a_ECB       euro area  2001-04-02   \n",
       "3      r171207a_BOE  united kingdom  2017-12-07   \n",
       "4      r170913a_ECB       euro area  2017-09-13   \n",
       "...             ...             ...         ...   \n",
       "7716   r171214a_SNB     switzerland  2017-12-14   \n",
       "7717   r991019a_BOJ           japan  1999-10-19   \n",
       "7718   r131122a_ECB       euro area  2013-11-22   \n",
       "7719  r970313a_FOMC   united states  1997-03-13   \n",
       "7720   r220428a_BOE  united kingdom  2022-04-28   \n",
       "\n",
       "                                                  title     author  is_gov  \\\n",
       "0                 Introductory remarks, news conference   maechler       0   \n",
       "1                                     Welcoming Remarks     powell       1   \n",
       "2     Consolidation of the payment systems industry:...     london       0   \n",
       "3     Making banks resolvable: the key to making res...     gracie       0   \n",
       "4                                Karl-Otto Pöhl Lecture      praet       0   \n",
       "...                                                 ...        ...     ...   \n",
       "7716              Introductory remarks, news conference   zurbrugg       0   \n",
       "7717                          On Recent Monetary Policy  yamaguchi       0   \n",
       "7718  Opening speech at the European Banking Congres...     draghi       1   \n",
       "7719                                                 we     rivlin       0   \n",
       "7720                                           Macropru    breeden       0   \n",
       "\n",
       "                                                   text  \n",
       "0     I would like to begin by reviewing development...  \n",
       "1     Good morning, and welcome. It is a great pleas...  \n",
       "2     I would first like to thank you for giving me ...  \n",
       "3     Resolution has come a long way since G20 Leade...  \n",
       "4     The monetary policy measures introduced by the...  \n",
       "...                                                 ...  \n",
       "7716  In my remarks today, I would like to address s...  \n",
       "7717  I am honored to be invited to this conference ...  \n",
       "7718  Ladies and gentlemen, Thank you for inviting m...  \n",
       "7719  I am extremely pleased to have the opportunity...  \n",
       "7720  Dimitri Demekas and Paul Tucker for helpful di...  \n",
       "\n",
       "[7721 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up initial data frame \n",
    "data_path = \"1430_data/speeches.csv\"\n",
    "# putting it into dataframe \n",
    "speeches = pd.read_csv(data_path)\n",
    "speeches = speeches.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "unique_countries = speeches['country'].unique()\n",
    "unique_countries\n",
    "speeches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d05071e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "speech_texts = speeches['text'].to_list()\n",
    "cleaned_speeches = []\n",
    "remove_index = []\n",
    "# iterating and extracting sentences\n",
    "i =0\n",
    "for speech in speech_texts:\n",
    "    extract = Extract(speech)\n",
    "    sentences = extract.extract_sentences()\n",
    "\n",
    "    if sentences==[]:\n",
    "      remove_index.append(i)\n",
    "    elif sentences!=[]:\n",
    "      cleaned_speeches.append(sentences)\n",
    "    i +=1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a7fdb165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reference</th>\n",
       "      <th>country</th>\n",
       "      <th>date</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>is_gov</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r191212c_SNB</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>2019-12-12</td>\n",
       "      <td>Introductory remarks, news conference</td>\n",
       "      <td>maechler</td>\n",
       "      <td>0</td>\n",
       "      <td>I would like to begin by reviewing development...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>r220524a_FOMC</td>\n",
       "      <td>united states</td>\n",
       "      <td>2022-05-24</td>\n",
       "      <td>Welcoming Remarks</td>\n",
       "      <td>powell</td>\n",
       "      <td>1</td>\n",
       "      <td>Good morning, and welcome. It is a great pleas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>r010402a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2001-04-02</td>\n",
       "      <td>Consolidation of the payment systems industry:...</td>\n",
       "      <td>london</td>\n",
       "      <td>0</td>\n",
       "      <td>I would first like to thank you for giving me ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>r171207a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2017-12-07</td>\n",
       "      <td>Making banks resolvable: the key to making res...</td>\n",
       "      <td>gracie</td>\n",
       "      <td>0</td>\n",
       "      <td>Resolution has come a long way since G20 Leade...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>r170913a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2017-09-13</td>\n",
       "      <td>Karl-Otto Pöhl Lecture</td>\n",
       "      <td>praet</td>\n",
       "      <td>0</td>\n",
       "      <td>The monetary policy measures introduced by the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7713</th>\n",
       "      <td>r171214a_SNB</td>\n",
       "      <td>switzerland</td>\n",
       "      <td>2017-12-14</td>\n",
       "      <td>Introductory remarks, news conference</td>\n",
       "      <td>zurbrugg</td>\n",
       "      <td>0</td>\n",
       "      <td>In my remarks today, I would like to address s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7714</th>\n",
       "      <td>r991019a_BOJ</td>\n",
       "      <td>japan</td>\n",
       "      <td>1999-10-19</td>\n",
       "      <td>On Recent Monetary Policy</td>\n",
       "      <td>yamaguchi</td>\n",
       "      <td>0</td>\n",
       "      <td>I am honored to be invited to this conference ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7715</th>\n",
       "      <td>r131122a_ECB</td>\n",
       "      <td>euro area</td>\n",
       "      <td>2013-11-22</td>\n",
       "      <td>Opening speech at the European Banking Congres...</td>\n",
       "      <td>draghi</td>\n",
       "      <td>1</td>\n",
       "      <td>Ladies and gentlemen, Thank you for inviting m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7716</th>\n",
       "      <td>r970313a_FOMC</td>\n",
       "      <td>united states</td>\n",
       "      <td>1997-03-13</td>\n",
       "      <td>we</td>\n",
       "      <td>rivlin</td>\n",
       "      <td>0</td>\n",
       "      <td>I am extremely pleased to have the opportunity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7717</th>\n",
       "      <td>r220428a_BOE</td>\n",
       "      <td>united kingdom</td>\n",
       "      <td>2022-04-28</td>\n",
       "      <td>Macropru</td>\n",
       "      <td>breeden</td>\n",
       "      <td>0</td>\n",
       "      <td>Dimitri Demekas and Paul Tucker for helpful di...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7718 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          reference         country        date  \\\n",
       "0      r191212c_SNB     switzerland  2019-12-12   \n",
       "1     r220524a_FOMC   united states  2022-05-24   \n",
       "2      r010402a_ECB       euro area  2001-04-02   \n",
       "3      r171207a_BOE  united kingdom  2017-12-07   \n",
       "4      r170913a_ECB       euro area  2017-09-13   \n",
       "...             ...             ...         ...   \n",
       "7713   r171214a_SNB     switzerland  2017-12-14   \n",
       "7714   r991019a_BOJ           japan  1999-10-19   \n",
       "7715   r131122a_ECB       euro area  2013-11-22   \n",
       "7716  r970313a_FOMC   united states  1997-03-13   \n",
       "7717   r220428a_BOE  united kingdom  2022-04-28   \n",
       "\n",
       "                                                  title     author  is_gov  \\\n",
       "0                 Introductory remarks, news conference   maechler       0   \n",
       "1                                     Welcoming Remarks     powell       1   \n",
       "2     Consolidation of the payment systems industry:...     london       0   \n",
       "3     Making banks resolvable: the key to making res...     gracie       0   \n",
       "4                                Karl-Otto Pöhl Lecture      praet       0   \n",
       "...                                                 ...        ...     ...   \n",
       "7713              Introductory remarks, news conference   zurbrugg       0   \n",
       "7714                          On Recent Monetary Policy  yamaguchi       0   \n",
       "7715  Opening speech at the European Banking Congres...     draghi       1   \n",
       "7716                                                 we     rivlin       0   \n",
       "7717                                           Macropru    breeden       0   \n",
       "\n",
       "                                                   text  \n",
       "0     I would like to begin by reviewing development...  \n",
       "1     Good morning, and welcome. It is a great pleas...  \n",
       "2     I would first like to thank you for giving me ...  \n",
       "3     Resolution has come a long way since G20 Leade...  \n",
       "4     The monetary policy measures introduced by the...  \n",
       "...                                                 ...  \n",
       "7713  In my remarks today, I would like to address s...  \n",
       "7714  I am honored to be invited to this conference ...  \n",
       "7715  Ladies and gentlemen, Thank you for inviting m...  \n",
       "7716  I am extremely pleased to have the opportunity...  \n",
       "7717  Dimitri Demekas and Paul Tucker for helpful di...  \n",
       "\n",
       "[7718 rows x 7 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "speeches = speeches.drop(index= remove_index)\n",
    "speeches = speeches.reset_index(drop=True)\n",
    "speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9413d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: (34, 768)\n",
      "speech: 0 encoded, avg time-0.4997861385345459 seconds \n",
      "output: (17, 768)\n",
      "speech: 1 encoded, avg time-0.30783402919769287 seconds \n",
      "output: (88, 768)\n",
      "speech: 2 encoded, avg time-0.4586190382639567 seconds \n",
      "output: (65, 768)\n",
      "speech: 3 encoded, avg time-0.47978848218917847 seconds \n",
      "output: (52, 768)\n",
      "speech: 4 encoded, avg time-0.4598729610443115 seconds \n",
      "output: (134, 768)\n",
      "speech: 5 encoded, avg time-0.5111297766367594 seconds \n",
      "output: (63, 768)\n",
      "speech: 6 encoded, avg time-0.5073113782065255 seconds \n",
      "output: (94, 768)\n",
      "speech: 7 encoded, avg time-0.5178166925907135 seconds \n",
      "output: (91, 768)\n",
      "speech: 8 encoded, avg time-0.5350361929999458 seconds \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m curr_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      8\u001b[0m curr_speech \u001b[38;5;241m=\u001b[39m cleaned_speeches[i] \n\u001b[0;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_speech\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m speech_vecs[i]\u001b[38;5;241m=\u001b[39m embeddings\n\u001b[1;32m     11\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/anaconda3/envs/text310/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:720\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, **kwargs)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[38;5;66;03m# fixes for #522 and #487 to avoid oom problems on gpu with large datasets\u001b[39;00m\n\u001b[1;32m    719\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m convert_to_numpy:\n\u001b[0;32m--> 720\u001b[0m                 embeddings \u001b[38;5;241m=\u001b[39m \u001b[43membeddings\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m         all_embeddings\u001b[38;5;241m.\u001b[39mextend(embeddings)\n\u001b[1;32m    724\u001b[0m all_embeddings \u001b[38;5;241m=\u001b[39m [all_embeddings[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39margsort(length_sorted_idx)]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# deriving vectors for each of the sentences \n",
    "speech_vecs = [np.zeros(shape=(len(cleaned_speeches[i]), 384)) for i in range(0,len(cleaned_speeches))]\n",
    "# iterating and deriving setence level embeddings \n",
    "num_iters = 100\n",
    "times = []\n",
    "for i in range(num_iters):\n",
    "    curr_time = time.time()\n",
    "    curr_speech = cleaned_speeches[i] \n",
    "    embeddings = model.encode(curr_speech)\n",
    "    speech_vecs[i]= embeddings\n",
    "    end_time = time.time()\n",
    "    it_time = curr_time - end_time\n",
    "    times.append(it_time)\n",
    "    print(f'speech: {i} encoded, avg time{np.mean(times)} seconds ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "854abee1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Inflation', 'Domestic Growth', 'Trade Balance',\n",
       "       'Value of Currency'], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# whether using subset and need to slice the embeddings list\n",
    "\n",
    "using_slice = True ## edit to false when using full dataset \n",
    "# slicing dataset \n",
    "if using_slice:\n",
    "    data_set = speech_vecs[:num_iters]\n",
    "    assert(len(data_set)== num_iters)\n",
    "\n",
    "elif using_slice == False:\n",
    "    data_set = speech_vecs\n",
    "\n",
    "# now generating embeddings for different topic areas \n",
    "key_sentences = pd.read_csv(\"key_vecs/1430_true_keys.csv\")\n",
    "topics = key_sentences['Factor'].unique()\n",
    "topic_sentence_dict = {topic:[] for topic in topics}\n",
    "for index, row in key_sentences.iterrows():\n",
    "    curr_topic = row['Factor']\n",
    "    curr_list = topic_sentence_dict[curr_topic]\n",
    "    curr_list.append(row['Sentence'])\n",
    "\n",
    "# map containing topic and key vector to compare to for similarity \n",
    "key_vecs = []\n",
    "\n",
    "for term in topics :\n",
    "    sentences = topic_sentence_dict[term]\n",
    "    out_encodings = model.encode(sentences=sentences)\n",
    "    mean_vec = np.mean(out_encodings, axis=0)\n",
    "\n",
    "    key_vecs.append((term, mean_vec))\n",
    "\n",
    "topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "504eb9de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3. 0. 0. 2. 0. 2. 2. 1. 1. 2. 1. 2. 1. 1. 1. 1. 1. 3. 3. 3. 0. 0. 3. 3.\n",
      " 3. 1. 3. 3. 2. 3. 0. 1. 0. 3. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 1. 0.\n",
      " 0. 0. 0. 3. 0. 3. 1. 1. 3. 1. 1. 1. 0. 3. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0.\n",
      " 1. 0. 1. 1. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 0. 3. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 2. 1. 0. 1. 0.\n",
      " 2. 3. 1. 3. 1. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "## finding the closest and labelling them \n",
    "\n",
    "label = LabelSimilar(key_vecs)\n",
    "list_topics = [np.zeros(data_set[i].shape[0], dtype=StringDType()) for i in range (0, len(data_set))]\n",
    "scores = [np.zeros(data_set[i].shape[0], dtype=np.float32) for i in range (0, len(data_set))]\n",
    "\n",
    "a = 0 \n",
    "for entry in data_set:\n",
    "    curr_mat = list_topics[a]\n",
    "    curr_score = scores[a]\n",
    "    a +=1\n",
    "    k = 0 \n",
    "    for sentence in entry:\n",
    "        dot_prod = label.cosine_similarity(sentence)\n",
    "        curr_mat[k] = dot_prod[0]\n",
    "        curr_score[k] = dot_prod[1]\n",
    "        k +=1\n",
    "        \n",
    "topics_map = {topic:i for (topic,i) in zip(topics, range(0,len(topics)))}\n",
    "topics_nums_arr =  [np.zeros(data_set[i].shape[0]) for i in range (0, len(data_set))]\n",
    "j = 0 \n",
    "for arr in list_topics:\n",
    "    topic_arr = topics_nums_arr[j]\n",
    "    j +=1 \n",
    "    for i in range(0, arr.shape[0]):\n",
    "        topic_arr[i] = topics_map[arr[i]]\n",
    "        \n",
    "print(topic_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab9f210b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54, 1)\n",
      "(54, 386)\n",
      "(100, 1)\n",
      "(100, 386)\n",
      "(947, 1)\n",
      "(947, 386)\n",
      "(33, 1)\n",
      "(33, 386)\n",
      "(126, 1)\n",
      "(126, 386)\n",
      "(185, 1)\n",
      "(185, 386)\n",
      "(17, 1)\n",
      "(17, 386)\n",
      "(110, 1)\n",
      "(110, 386)\n",
      "(42, 1)\n",
      "(42, 386)\n",
      "(128, 1)\n",
      "(128, 386)\n"
     ]
    }
   ],
   "source": [
    "# initial grouping of vectors \n",
    "for i in range(0, len(data_set)):\n",
    "    curr_matrix = data_set[i]\n",
    "    curr_labels = topics_nums_arr[i]\n",
    "    curr_labels= curr_labels.reshape((curr_labels.shape[0],1))\n",
    "    print(curr_labels.shape)\n",
    "    curr_scores = scores[i]\n",
    "    curr_scores = curr_scores.reshape((curr_scores.shape[0],1))\n",
    "    data_set[i] = np.concatenate((curr_matrix, curr_labels, curr_scores), axis=1)\n",
    "    print(data_set[i].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14224a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieving indexes of sentences matching the groupings by topic \n",
    "\n",
    "topic_group_index = []\n",
    "for i in range(0, len(data_set)):\n",
    "    curr_matrix = data_set[i]\n",
    "    m1 = np.where(curr_matrix[:,384]==0)\n",
    "    m2 = np.where(curr_matrix[:,384] ==1)\n",
    "    m3 = np.where(curr_matrix[:,384]==2)\n",
    "    m4 = np.where(curr_matrix[:,384]== 3)\n",
    "    m5 = np.where(curr_matrix[:,384]== 4)\n",
    "    m6 = np.where(curr_matrix[:,384]== 5)\n",
    "    m7 = np.where(curr_matrix[:,384]== 6)\n",
    "    m8 = np.where(curr_matrix[:,384]== 7)\n",
    "    m9 = np.where(curr_matrix[:,384]== 8)\n",
    "    m10 = np.where(curr_matrix[:,384]== 9)\n",
    "    topic_indices = [m1, m2,m3,m4, m5,m6,m7,m8,m9,m10]\n",
    "    topic_group_index.append(topic_indices)\n",
    "\n",
    "len(topic_group_index[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a70a4ab5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/prottoyachowdhury/anaconda3/envs/text310/bin/python'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys \n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da51add7",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_sentences = [[] for i in range(0, len(topics))]\n",
    "for i in range(0, len(data_set)):\n",
    "    curr_text = cleaned_speeches[i]\n",
    "    curr_indices = topic_group_index[i]\n",
    "    for k in range(0, len(topics)):\n",
    "        indices:np.ndarray = curr_indices[k][0]\n",
    "        \n",
    "        related_sentences = [curr_text[j] for j in indices.astype(int)]\n",
    "        topic_sentences[k].append(related_sentences)\n",
    "\n",
    "sorted_data = pd.DataFrame(topic_sentences).T\n",
    "sorted_data = sorted_data.rename(columns={i: topics[i] for i in range(0, len(topics))})\n",
    "sorted_data['date'] = pd.to_datetime(speeches['date'][0:sorted_data.shape[0]])\n",
    "sorted_data['country'] = speeches['country'][0:sorted_data.shape[0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8afdf117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assembling the dataframes that will contain data on the yields \n",
    "# declaring them as list then mapping through hashmap \n",
    "\n",
    "rate_df_list = []\n",
    "for file in os.listdir('1430_rates'):\n",
    "    rate_df = pd.read_csv('1430_rates/'+file)\n",
    "    rate_df_list.append(rate_df)\n",
    "\n",
    "country_names = [file.split(\" \")[0].lower() for file in os.listdir('1430_rates')]\n",
    "country_names[1] = 'euro area'\n",
    "country_names[4] = 'united kingdom'\n",
    "country_names[5] = 'united states'\n",
    "\n",
    "# Declaring country and interest rate mapper \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "637ff41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating percent change values in yields \n",
    "\n",
    "# Columns to drop \n",
    "drop_cols = ['Unnamed: 0', 'date']\n",
    "\n",
    "def df_convert_pct(df: pd.DataFrame, period:int)-> pd.DataFrame:\n",
    "    dates = df['date'].copy(deep=True) \n",
    "    cleaned_df = df.drop(columns=drop_cols)\n",
    "    dates = pd.to_datetime(dates)\n",
    "    percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
    "    \n",
    "    percent_change = percent_change.iloc[period:, :].reset_index(drop=True)\n",
    "    num_rows = percent_change.shape[0]\n",
    "    #appending dates to the final df \n",
    "    percent_change['date'] = dates[0:num_rows]\n",
    "    return percent_change\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7b6c1154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_12230/2391588030.py:10: FutureWarning: The default fill_method='pad' in DataFrame.pct_change is deprecated and will be removed in a future version. Either fill in any non-leading NA values prior to calling pct_change or specify 'fill_method=None' to not fill NA values.\n",
      "  percent_change = cleaned_df.pct_change(periods = period ).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10y</th>\n",
       "      <th>3m_ib</th>\n",
       "      <th>3m_bb</th>\n",
       "      <th>24h_immed</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.045568</td>\n",
       "      <td>-0.021329</td>\n",
       "      <td>-0.073345</td>\n",
       "      <td>-0.046105</td>\n",
       "      <td>1990-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000868</td>\n",
       "      <td>-0.025985</td>\n",
       "      <td>-0.074530</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1991-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.013010</td>\n",
       "      <td>-0.011188</td>\n",
       "      <td>-0.163062</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1991-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.035149</td>\n",
       "      <td>-0.011314</td>\n",
       "      <td>-0.106163</td>\n",
       "      <td>-0.037500</td>\n",
       "      <td>1991-03-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.020947</td>\n",
       "      <td>-0.056338</td>\n",
       "      <td>-0.103203</td>\n",
       "      <td>-0.049351</td>\n",
       "      <td>1991-04-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>-0.085408</td>\n",
       "      <td>-0.006849</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.013986</td>\n",
       "      <td>2023-11-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>-0.009315</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2023-12-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>-0.001688</td>\n",
       "      <td>-0.002299</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>-0.023183</td>\n",
       "      <td>0.002304</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-02-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>0.055130</td>\n",
       "      <td>0.004598</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2024-03-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          10y     3m_ib     3m_bb  24h_immed       date\n",
       "0   -0.045568 -0.021329 -0.073345  -0.046105 1990-12-01\n",
       "1    0.000868 -0.025985 -0.074530   0.000000 1991-01-01\n",
       "2   -0.013010 -0.011188 -0.163062   0.000000 1991-02-01\n",
       "3   -0.035149 -0.011314 -0.106163  -0.037500 1991-03-01\n",
       "4   -0.020947 -0.056338 -0.103203  -0.049351 1991-04-01\n",
       "..        ...       ...       ...        ...        ...\n",
       "395 -0.085408 -0.006849       NaN   0.013986 2023-11-01\n",
       "396 -0.009315  0.000000       NaN   0.000000 2023-12-01\n",
       "397 -0.001688 -0.002299       NaN   0.000000 2024-01-01\n",
       "398 -0.023183  0.002304       NaN   0.000000 2024-02-01\n",
       "399  0.055130  0.004598       NaN   0.000000 2024-03-01\n",
       "\n",
       "[400 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating time lagged df lists for rates across all countries and regions \n",
    "one_month = {country_names[i]: df_convert_pct(rate_df_list[i], 1) for i in range(0, len(rate_df_list))}\n",
    "two_month = {country_names[i]:df_convert_pct(rate_df_list[i],2) for i in range(0,len(rate_df_list))}\n",
    "three_month = {country_names[i]: df_convert_pct(rate_df_list[i],3) for i in range(0, len(rate_df_list))}\n",
    "five_month = {country_names[i]: df_convert_pct(rate_df_list[i],5) for i in range(0, len(rate_df_list))}\n",
    "six_month = {country_names[i]: df_convert_pct(rate_df_list[i],6) for i in range(0, len(rate_df_list))}\n",
    "seven_month = {country_names[i]: df_convert_pct(rate_df_list[i],7) for i in range(0, len(rate_df_list))}\n",
    "eigth_month = {country_names[i]: df_convert_pct(rate_df_list[i],8) for i in range(0, len(rate_df_list))}\n",
    "nine_month = {country_names[i]: df_convert_pct(rate_df_list[i],9) for i in range(0, len(rate_df_list))}\n",
    "ten_month = {country_names[i]: df_convert_pct(rate_df_list[i],10) for i in range(0, len(rate_df_list))}\n",
    "eleven_month = {country_names[i]: df_convert_pct(rate_df_list[i],11) for i in range(0, len(rate_df_list))}\n",
    "twelve_month = {country_names[i]: df_convert_pct(rate_df_list[i],12) for i in range(0, len(rate_df_list))}\n",
    "\n",
    "one_month['australia']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a862062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42477644492912514,\n",
       " 0.22181146025877996,\n",
       " 2.1999999999999997,\n",
       " 0.8226371848456189,\n",
       " 3.0,\n",
       " -0.20143884892086317,\n",
       " -0.9320754716981132,\n",
       " 0.13547237076648844,\n",
       " 0.034482758620689724,\n",
       " 23.804337794185432]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating mapping function that maps lagged rates to the speeches \n",
    "sorted_dates = sorted_data['date'].tolist()\n",
    "sorted_countries = sorted_data['country'].tolist()\n",
    "\n",
    "# Function to map rates \n",
    "\n",
    "def rate_mapper(column_name: str, lagged_rates:dict, lag_amount:str)->pd.DataFrame:\n",
    "    rate_list = []\n",
    "    ## iterating through list of dates and countries \n",
    "    for date, country in zip(sorted_dates, sorted_countries):\n",
    "        curr_df:pd.DataFrame = lagged_rates[country]\n",
    "        month = date.month \n",
    "        year = date.year\n",
    "        try:\n",
    "            row_slice = curr_df.loc[(curr_df['date'].dt.month == month) & (curr_df['date'].dt.year==year)]\n",
    "          \n",
    "            val_to_add = row_slice[column_name].values\n",
    "            rate_list.append(float(val_to_add[0]))\n",
    "        except:\n",
    "            rate_list.append(np.nan)\n",
    "            print('nan value')\n",
    "    \n",
    "    sorted_data[lag_amount+\"_\"+column_name] = rate_list\n",
    "    return rate_list\n",
    "    \n",
    "\n",
    "rate_mapper('10y', one_month, 'one_month')\n",
    "sorted_data\n",
    "\n",
    "\n",
    "rate_mapper('10y', three_month, 'three_month')\n",
    "rate_mapper('10y', six_month, 'six_month')\n",
    "rate_mapper('10y', twelve_month, 'twelve_month')\n",
    "rate_mapper('3m_ib', one_month, 'one_month')\n",
    "rate_mapper('3m_ib', two_month, 'two_month')\n",
    "rate_mapper('3m_ib', three_month, 'three_month')\n",
    "rate_mapper('3m_ib', six_month, 'six_month')\n",
    "rate_mapper('3m_ib', twelve_month, 'twelve_month')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "db7c35d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['On Christmas Eve 1818 \"Silent Night\" was performed for the first time, in a chapel in Oberndorf, near Salzburg and subsequently made its way throughout Europe and to the rest of the world.',\n",
       " \"For this reason, I had welcomed the European Commission's decision to organise a public hearing on the issue, which took place last Wednesday.\",\n",
       " 'However, I am confident that the European Commission will make this possible through a legislative proposal.',\n",
       " 'This would give banks a range of options to choose from as regards the processing of card payments.',\n",
       " 'Third, various market initiatives have developed implementation standards and specifications for the various domains of card payments.',\n",
       " 'However, I personally believe that the European cards market would also benefit from the power of choice.',\n",
       " 'The Eurosystem has been calling for an additional European card scheme for four years.',\n",
       " 'I would strongly urge the appropriate authorities to work on this issue so that the clarity needed at the European level can be provided.',\n",
       " 'An additional European card scheme - which is as safe and efficient as the best performing schemes today - is a necessary element of an integrated and competitive cards market.',\n",
       " 'Europe is still a patchwork of national online markets, and Europeans are prevented from enjoying the benefits of a single digital market.',\n",
       " 'Traditional payment instruments still dominate payments for e-commerce.',\n",
       " 'However, they were not designed to cope with the needs of the online world and often cannot be used for cross-border transactions.',\n",
       " 'In all but a few countries, card transactions based on secure payment protocols and e-payments based on online banking are still in the minority or are only used for domestic transactions.',\n",
       " 'Therefore, there is a clear need for efficient, low cost, secure and readily available online payment solutions throughout Europe - and ideally beyond.',\n",
       " 'The potential for online payment solutions is great enough to offer scope both for secure card payments and for e-payments based on online banking.',\n",
       " 'Both increased security for \"card-not-present\" transactions and the introduction, throughout Europe, of e-payments based on online banking will provide more payment options for users.',\n",
       " 'The risk-based approach applied by individual banks may not be sufficient to achieve the level of security that is required at the aggregate industry level.',\n",
       " 'This should ensure that sensitive account information can no longer be copied.',\n",
       " 'We have recently launched the idea to set up a Forum for Security Issues and have received a positive feedback from banks.',\n",
       " 'In addition it could enable the relevant authorities, including banking supervisors and central banks, to align their expectations on security.',\n",
       " 'The Eurosystem is currently investigating what could be the precise mandate and composition of such a forum.']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_data.iloc[0,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "634d14c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compling list of strings into one string \n",
    "subset_text_df = sorted_data.iloc[:, 0:4]\n",
    "\n",
    "def string_join(list_str: list[str]):\n",
    "    out = \"\"\n",
    "    for i in range(0, len(list_str)):\n",
    "        curr_string = list_str[i]\n",
    "        out += curr_string\n",
    "    return out \n",
    "\n",
    "\n",
    "file_path = \"clean_merged_data/\"\n",
    "subset_text_df.to_csv(file_path+\"/initial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe544651",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentiment_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msentiment_df\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sentiment_df' is not defined"
     ]
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ad9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_82322/1109739794.py:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  curr_list = row[i]\n",
      "/var/folders/xh/f6kdyynj1c72w_k6zhvc8t1h0000gn/T/ipykernel_82322/1109739794.py:30: FutureWarning: Series.__setitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To set a value by position, use `ser.iloc[pos] = value`\n",
      "  row[i] = np.mean(values)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speech 1 sentiment done \n",
      ", time:1.1761541366577148 sec\n",
      "speech 2 sentiment done \n",
      ", time:0.8228631019592285 sec\n",
      "speech 3 sentiment done \n",
      ", time:1.1783568859100342 sec\n",
      "speech 4 sentiment done \n",
      ", time:1.5702719688415527 sec\n",
      "speech 5 sentiment done \n",
      ", time:1.914783000946045 sec\n",
      "speech 6 sentiment done \n",
      ", time:1.5502479076385498 sec\n",
      "speech 7 sentiment done \n",
      ", time:2.0138680934906006 sec\n",
      "speech 8 sentiment done \n",
      ", time:0.5818321704864502 sec\n",
      "speech 9 sentiment done \n",
      ", time:7.019974946975708 sec\n",
      "speech 10 sentiment done \n",
      ", time:1.5091650485992432 sec\n"
     ]
    }
   ],
   "source": [
    "# Now using the finbert model to create vector outputs for sentiment \n",
    "sentiment_df = subset_text_df.copy()\n",
    "sentiment_df\n",
    "\n",
    "def sentiment_map(sentiment:tuple):\n",
    "    label = sentiment['label']\n",
    "    score = sentiment['score']\n",
    "    if label == 'positive':\n",
    "        return 30 * score \n",
    "    elif label == 'neutral':\n",
    "        return 20 * score \n",
    "    elif label == 'negative':\n",
    "        return 10 * score \n",
    "    \n",
    "\n",
    "sentiment_df.iloc[0,0]\n",
    "count =1\n",
    "for index, row in sentiment_df.iterrows():\n",
    "    start = time.time()\n",
    "    for i in range(0, len(row)):\n",
    "       \n",
    "        values = []\n",
    "        curr_list = row[i]\n",
    "        for j in range(len(curr_list)):\n",
    "            out = pipe(curr_list[j], num_workers=10)[0]\n",
    "       \n",
    "            val_add = sentiment_map(out)\n",
    "            values.append(val_add)\n",
    "     \n",
    "        row[i] = np.mean(values)\n",
    "        \n",
    "   \n",
    "    end = time.time()\n",
    "    print(f'speech {count} sentiment done \\n, time:{end-start} sec')\n",
    "    count +=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c8635",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_csv('1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ea74ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Inflation</th>\n",
       "      <th>Domestic Growth</th>\n",
       "      <th>Trade Balance</th>\n",
       "      <th>Value of Currency</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.893559</td>\n",
       "      <td>21.883068</td>\n",
       "      <td>18.465516</td>\n",
       "      <td>16.366768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.93239</td>\n",
       "      <td>16.981548</td>\n",
       "      <td>16.572093</td>\n",
       "      <td>15.72388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.645284</td>\n",
       "      <td>13.283747</td>\n",
       "      <td>16.620332</td>\n",
       "      <td>13.054433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>17.127409</td>\n",
       "      <td>16.278965</td>\n",
       "      <td>18.98627</td>\n",
       "      <td>18.149541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15.819385</td>\n",
       "      <td>16.777114</td>\n",
       "      <td>15.697898</td>\n",
       "      <td>13.409928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14.477584</td>\n",
       "      <td>16.069378</td>\n",
       "      <td>14.656512</td>\n",
       "      <td>13.219095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14.312674</td>\n",
       "      <td>16.832058</td>\n",
       "      <td>15.810309</td>\n",
       "      <td>16.060913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>12.838652</td>\n",
       "      <td>21.087051</td>\n",
       "      <td>19.533621</td>\n",
       "      <td>14.108101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.575795</td>\n",
       "      <td>16.517491</td>\n",
       "      <td>18.001249</td>\n",
       "      <td>17.616931</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>14.760858</td>\n",
       "      <td>15.152376</td>\n",
       "      <td>14.420749</td>\n",
       "      <td>15.980881</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Inflation Domestic Growth Trade Balance Value of Currency\n",
       "0  22.893559       21.883068     18.465516         16.366768\n",
       "1   15.93239       16.981548     16.572093          15.72388\n",
       "2  12.645284       13.283747     16.620332         13.054433\n",
       "3  17.127409       16.278965      18.98627         18.149541\n",
       "4  15.819385       16.777114     15.697898         13.409928\n",
       "5  14.477584       16.069378     14.656512         13.219095\n",
       "6  14.312674       16.832058     15.810309         16.060913\n",
       "7  12.838652       21.087051     19.533621         14.108101\n",
       "8  16.575795       16.517491     18.001249         17.616931\n",
       "9  14.760858       15.152376     14.420749         15.980881"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10848aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_data.iloc[:, 0:4]= sentiment_df\n",
    "sorted_data\n",
    "file_name = 'cleaned_sentiment.csv'\n",
    "sorted_data.to_csv(file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "text310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
